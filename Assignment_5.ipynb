{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV-2Ik_kfxmV"
   },
   "source": [
    "# Stanford 40\n",
    "### Download the data\n",
    "You can see the zip files if you click the Files tab (looks like a folder symbol on the left of the screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HhJtnW9eCTHd",
    "outputId": "c186014b-96ac-407b-d852-1a2b2a131abb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget http://vision.stanford.edu/Datasets/Stanford40_JPEGImages.zip\n",
    "!wget http://vision.stanford.edu/Datasets/Stanford40_ImageSplits.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "za-O1jEGgCLg"
   },
   "source": [
    "### Unzip it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNpYOPOcCkMZ"
   },
   "outputs": [],
   "source": [
    "!unzip Stanford40_JPEGImages.zip -d Stanford40/\n",
    "!unzip Stanford40_ImageSplits.zip -d Stanford40/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzT70V9sghkX"
   },
   "source": [
    "## Read the train and test splits, combine them and make better splits to help training networks easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xLaWDCdDO0Y",
    "outputId": "48152490-73c1-493d-8f0c-0b2ae88ef7eb"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "keep_stanford40 = [\"applauding\", \"climbing\", \"drinking\", \"jumping\", \"pouring_liquid\", \"riding_a_bike\", \"riding_a_horse\", \n",
    "        \"running\", \"shooting_an_arrow\", \"smoking\", \"throwing_frisby\", \"waving_hands\"]\n",
    "with open('Stanford40/ImageSplits/train.txt', 'r') as f:\n",
    "    # We won't use these splits but split them ourselves\n",
    "    train_files = [file_name for file_name in list(map(str.strip, f.readlines())) if '_'.join(file_name.split('_')[:-1]) in keep_stanford40]\n",
    "    train_labels = ['_'.join(name.split('_')[:-1]) for name in train_files]\n",
    "\n",
    "with open('Stanford40/ImageSplits/test.txt', 'r') as f:\n",
    "    # We won't use these splits but split them ourselves\n",
    "    test_files = [file_name for file_name in list(map(str.strip, f.readlines())) if '_'.join(file_name.split('_')[:-1]) in keep_stanford40]\n",
    "    test_labels = ['_'.join(name.split('_')[:-1]) for name in test_files]\n",
    "\n",
    "# Combine the splits and split for keeping more images in the training set than the test set.\n",
    "all_files = train_files + test_files\n",
    "all_labels = train_labels + test_labels\n",
    "train_files, test_files = train_test_split(all_files, test_size=0.1,random_state=0, stratify=all_labels)\n",
    "train_labels = ['_'.join(name.split('_')[:-1]) for name in train_files]\n",
    "test_labels = ['_'.join(name.split('_')[:-1]) for name in test_files]\n",
    "print(f'Train files ({len(train_files)}):\\n\\t{train_files}')\n",
    "print(f'Train labels ({len(train_labels)}):\\n\\t{train_labels}\\n'\\\n",
    "      f'Train Distribution:{list(Counter(sorted(train_labels)).items())}\\n')\n",
    "print(f'Test files ({len(test_files)}):\\n\\t{test_files}')\n",
    "print(f'Test labels ({len(test_labels)}):\\n\\t{test_labels}\\n'\\\n",
    "      f'Test Distribution:{list(Counter(sorted(test_labels)).items())}\\n')\n",
    "action_categories = sorted(list(set(train_labels)))\n",
    "print(f'Action categories ({len(action_categories)}):\\n{action_categories}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEYoDpMPifiJ"
   },
   "source": [
    "### Visualize a photo from the training files and also print its label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
    },
    "id": "qHE9lIZ0K-Ht",
    "outputId": "cf8394b9-ebf0-4606-8f7c-c3f606049c4c"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "image_no = 234  # change this to a number between [0, 1200] and you can see a different training image\n",
    "img = cv2.imread(f'Stanford40/JPEGImages/{train_files[image_no]}')\n",
    "print(f'An image with the label - {train_labels[image_no]}')\n",
    "cv2_imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMJz1Mr8ATgF"
   },
   "source": [
    "# Human Motion Database 51 (HMDB51)\n",
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hMr1xCPdATzQ",
    "outputId": "5aa3e009-4a79-42f8-f867-f29a54c07503"
   },
   "outputs": [],
   "source": [
    "!pip install av\n",
    "! wget https://raw.githubusercontent.com/pytorch/vision/6de158c473b83cf43344a0651d7c01128c7850e6/references/video_classification/transforms.py\n",
    "# Download HMDB51 data and splits from serre lab website\n",
    "! wget http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar\n",
    "! wget http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/test_train_splits.rar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Id3yXmQzBSiC"
   },
   "source": [
    "# Extract and organize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nq6cxDXOBUeB",
    "outputId": "e9e8db2a-a063-4137-9106-b21432c36739"
   },
   "outputs": [],
   "source": [
    "! mkdir -p video_data test_train_splits\n",
    "! unrar e test_train_splits.rar test_train_splits\n",
    "! rm test_train_splits.rar\n",
    "! unrar e hmdb51_org.rar \n",
    "! rm hmdb51_org.rar\n",
    "! mv *.rar video_data\n",
    "import os\n",
    "keep_hmdb51 = [\"clap\", \"climb\", \"drink\", \"jump\", \"pour\", \"ride_bike\", \"ride_horse\", \n",
    "        \"run\", \"shoot_bow\", \"smoke\", \"throw\", \"wave\"]\n",
    "for files in os.listdir('video_data'):\n",
    "    foldername = files.split('.')[0]\n",
    "    if foldername in keep_hmdb51:\n",
    "      # extract only the relevant classes for the assignment.\n",
    "      os.system(\"mkdir -p video_data/\" + foldername)\n",
    "      os.system(\"unrar e video_data/\"+ files + \" video_data/\"+foldername)\n",
    "\n",
    "! rm video_data/*.rar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvcM1mKkCDXR"
   },
   "source": [
    "# Load data into dataloaders with necessary transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xpqfKEmjCDmF",
    "outputId": "99aa3bf0-98d6-4045-8c99-621afacd93ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:29<00:00,  3.30it/s]\n",
      "/usr/local/lib/python3.9/dist-packages/torchvision/datasets/video_utils.py:219: UserWarning: There aren't enough frames in the current video to get a clip for the given clip length and frames between clips. The video (and potentially others) will be skipped.\n",
      "  warnings.warn(\n",
      "100%|██████████| 97/97 [00:31<00:00,  3.12it/s]\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch\n",
    "import transforms as T\n",
    "\n",
    "torch.manual_seed(97)\n",
    "num_frames = 16\n",
    "clip_steps = 2\n",
    "batch_size = 16\n",
    "\n",
    "transform = transforms.Compose([ T.ToFloatTensorInZeroOne(),\n",
    "                                 T.Resize((200, 200)),\n",
    "                                 T.RandomCrop((172, 172))])\n",
    "transform_test = transforms.Compose([                           \n",
    "                                 T.ToFloatTensorInZeroOne(),\n",
    "                                 T.Resize((200, 200)),\n",
    "                                 T.CenterCrop((172, 172))])\n",
    "\n",
    "\n",
    "hmdb51_train = torchvision.datasets.HMDB51('video_data/', 'test_train_splits/', num_frames, frame_rate=5,\n",
    "                                                step_between_clips = clip_steps, fold=1, train=True,\n",
    "                                                transform=transform, num_workers=2)\n",
    "\n",
    "hmdb51_test = torchvision.datasets.HMDB51('video_data/', 'test_train_splits/', num_frames, frame_rate=5,\n",
    "                                                step_between_clips = clip_steps, fold=1, train=False,\n",
    "                                                transform=transform_test, num_workers=2)\n",
    "\n",
    "train_loader = DataLoader(hmdb51_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(hmdb51_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnddUkZrEmBT"
   },
   "source": [
    "# Let's print the data shape with batch size 16 and 16 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "md9mJUCBEmRA",
    "outputId": "c5c16ddd-171c-4587-e149-d5cca5cadb0c"
   },
   "outputs": [],
   "source": [
    "for data, _, labels in train_loader:\n",
    "  print(data.shape)  # 16-batch size, 3-channels, 16-frames, 172x172-crop\n",
    "  print(labels)  # 12 classes [0-11]\n",
    "  break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
